{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook serves as a walkthrough for training with trapper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the correct path\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "from jury import Jury\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trapper.training.train import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# You can customize your logger below.\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EXPERIMENT_NAME = \"roberta-base-training-example\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "EXPERIMENTS_DIR = os.path.join(WORKING_DIR, \"examples/experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Some useful helper functions to ease training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_from_task(path: str, task: str):\n",
    "    task = \"unnamed-task\" if task is None else task\n",
    "    return path.format(task=task)\n",
    "\n",
    "def start_experiment(config: str, task: str, ext_vars: Dict[str, str]):\n",
    "    result = run_experiment(\n",
    "        config_path=config,\n",
    "        ext_vars=ext_vars,\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TASK = \"question-answering\"\n",
    "TASK_DIR = get_dir_from_task(os.path.join(EXPERIMENTS_DIR, \"{task}\"), task=TASK)\n",
    "DATASET_DIR = os.path.join(TASK_DIR, \"datasets\")\n",
    "EXPERIMENT_DIR = os.path.join(TASK_DIR, EXPERIMENT_NAME)\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_DIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(EXPERIMENT_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ext_vars = {\n",
    "    # Used to feed the jsonnet config file with file paths\n",
    "    \"OUTPUT_PATH\": OUTPUT_DIR,\n",
    "    \"CHECKPOINT_PATH\": CHECKPOINT_DIR\n",
    "}\n",
    "\n",
    "CONFIG_PATH = os.path.join(TASK_DIR, \"experiment.jsonnet\")  # default experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = start_experiment(\n",
    "    config=CONFIG_PATH,\n",
    "    task=TASK,\n",
    "    ext_vars=ext_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, usage of pipeline for inference is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trapper.pipelines.question_answering_pipeline import SquadQuestionAnsweringPipeline\n",
    "from trapper.pipelines.pipeline import create_pipeline_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Some helper functions for inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(samples: List[Dict], path: str):\n",
    "    with open(path, \"w\") as jf:\n",
    "        json.dump(samples, jf)\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\") as jf:\n",
    "        return json.load(jf)\n",
    "\n",
    "\n",
    "def prepare_samples(data: Union[str, Dict]):\n",
    "    if isinstance(data, str):\n",
    "        data = load_json(data)\n",
    "    data = data[\"data\"]\n",
    "    qa_samples = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = {}\n",
    "                sample[\"context\"] = paragraph[\"context\"]\n",
    "                sample[\"question\"] = qa[\"question\"]\n",
    "                sample[\"gold_answers\"] = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
    "                qa_samples.append(sample)\n",
    "\n",
    "    return qa_samples\n",
    "\n",
    "\n",
    "def prepare_samples_for_pipeline(samples: List[Dict]):\n",
    "    pipeline_samples = deepcopy(samples)\n",
    "    for sample in pipeline_samples:\n",
    "        sample.pop(\"gold_answers\")\n",
    "    return pipeline_samples\n",
    "\n",
    "\n",
    "def predict(pipeline, samples, **kwargs):\n",
    "    pipeline_samples = prepare_samples_for_pipeline(samples)\n",
    "    predictions = pipeline(pipeline_samples, **kwargs)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        samples[i][\"predicted_answer\"] = prediction[0][\"answer\"].text\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DEV = \"/home/devrimcavusoglu/lab/bb/nqg/datasets/squad/squad-qa/dev-v1.1.json\"\n",
    "EXPORT_PATH = \"/home/devrimcavusoglu/lab/bb/nqg/notebooks/qa-predictions_pipeline.json\"\n",
    "\n",
    "PRETRAINED_MODEL_PATH = \"/home/devrimcavusoglu/lab/bb/nqg/models/question_answering/roberta-large_ep_5_ebs_384_lr_5e-05-v1\"\n",
    "EXPERIMENT_CONFIG = os.path.join(PRETRAINED_MODEL_PATH, \"experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_pipeline = create_pipeline_from_checkpoint(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    experiment_config_path=EXPERIMENT_CONFIG,\n",
    "    task=\"squad-question-answering\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(SQUAD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(qa_pipeline, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(predictions, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [sample[\"gold_answers\"] for sample in predictions]\n",
    "hypotheses = [sample[\"predicted_answer\"] for sample in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury = Jury(metrics=[\"squad_f1\", \"squad_em\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury.evaluate(references=references, predictions=hypotheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
