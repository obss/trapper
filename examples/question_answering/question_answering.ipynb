{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook serves as a walkthrough for training with trapper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Setting up the correct path\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from jury import Jury\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trapper.training.train import run_experiment\n",
    "from trapper.common.notebook_utils import download_from_url, load_json, save_json\n",
    "from trapper.common.notebook_utils.prepare_data import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/devrimcavusoglu/.local/bin/datasets-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/datasets_cli.py\", line 33, in main\n",
      "    service.run()\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/test.py\", line 117, in run\n",
      "    module = dataset_module_factory(path)\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/load.py\", line 1146, in dataset_module_factory\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Couldn't find a dataset script at /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/squad_qa_test_fixture/dev.json/dev.json.py or any data file in the same directory. Couldn't find 'squad_qa_test_fixture/dev.json' on the Hugging Face Hub either: FileNotFoundError: Dataset 'squad_qa_test_fixture/dev.json' doesn't exist on the Hub\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "100%|██████████| 2/2 [00:00<00:00, 14691.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2497.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1151.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing builder 'qa_test_fixture' (1/1)\n",
      "Downloading and preparing dataset squad_test_fixture/qa_test_fixture (download: 10.43 KiB, generated: 9.62 KiB, post-processed: Unknown size, total: 20.05 KiB) to /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e...\n",
      "Dataset squad_test_fixture downloaded and prepared to /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e. Subsequent calls will reuse this data.\n",
      "Dataset Infos file saved at squad_qa_test_fixture/dataset_infos.json\n",
      "Test successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/devrimcavusoglu/.local/bin/datasets-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/datasets_cli.py\", line 33, in main\n",
      "    service.run()\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/test.py\", line 117, in run\n",
      "    module = dataset_module_factory(path)\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/load.py\", line 1146, in dataset_module_factory\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Couldn't find a dataset script at /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/squad_qa_test_fixture/train.json/train.json.py or any data file in the same directory. Couldn't find 'squad_qa_test_fixture/train.json' on the Hugging Face Hub either: FileNotFoundError: Dataset 'squad_qa_test_fixture/train.json' doesn't exist on the Hub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Summary Results ================================\n",
      "cache_dev.json failed.\n",
      "cache_train.json failed.\n",
      "cache_dataset_infos.json failed.\n",
      "============================= 1 success, 3 failure =============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/devrimcavusoglu/.local/bin/datasets-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/datasets_cli.py\", line 33, in main\n",
      "    service.run()\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/commands/test.py\", line 117, in run\n",
      "    module = dataset_module_factory(path)\n",
      "  File \"/home/devrimcavusoglu/.local/lib/python3.8/site-packages/datasets/load.py\", line 1146, in dataset_module_factory\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Couldn't find a dataset script at /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/squad_qa_test_fixture/dataset_infos.json/dataset_infos.json.py or any data file in the same directory. Couldn't find 'squad_qa_test_fixture/dataset_infos.json' on the Hugging Face Hub either: FileNotFoundError: Dataset 'squad_qa_test_fixture/dataset_infos.json' doesn't exist on the Hub\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devrimcavusoglu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# You can customize your logger below.\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EXPERIMENT_NAME = \"roberta-base-training-example\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(WORKING_DIR))\n",
    "EXPERIMENT_DIR = os.path.join(WORKING_DIR, EXPERIMENT_NAME)\n",
    "CONFIG_PATH = os.path.join(WORKING_DIR, \"experiment.jsonnet\")  # default experiment params\n",
    "\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_DIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(EXPERIMENT_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 13:40:10,422 | INFO : type = default\n",
      "2021-11-03 13:40:10,422 | INFO : pretrained_model_name_or_path = roberta-base\n",
      "2021-11-03 13:40:10,422 | INFO : train_split_name = train\n",
      "2021-11-03 13:40:10,423 | INFO : dev_split_name = validation\n",
      "2021-11-03 13:40:10,424 | INFO : label_mapper = None\n",
      "2021-11-03 13:40:10,425 | INFO : compute_metrics = None\n",
      "2021-11-03 13:40:10,425 | INFO : no_grad = None\n",
      "2021-11-03 13:40:10,426 | INFO : args.type = default\n",
      "2021-11-03 13:40:10,427 | INFO : args.output_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/checkpoints\n",
      "2021-11-03 13:40:10,427 | INFO : args.overwrite_output_dir = False\n",
      "2021-11-03 13:40:10,427 | INFO : args.do_train = True\n",
      "2021-11-03 13:40:10,428 | INFO : args.do_eval = True\n",
      "2021-11-03 13:40:10,428 | INFO : args.do_predict = False\n",
      "2021-11-03 13:40:10,429 | INFO : args.evaluation_strategy = steps\n",
      "2021-11-03 13:40:10,429 | INFO : args.prediction_loss_only = False\n",
      "2021-11-03 13:40:10,430 | INFO : args.per_device_train_batch_size = 2\n",
      "2021-11-03 13:40:10,430 | INFO : args.per_device_eval_batch_size = 2\n",
      "2021-11-03 13:40:10,431 | INFO : args.per_gpu_train_batch_size = None\n",
      "2021-11-03 13:40:10,431 | INFO : args.per_gpu_eval_batch_size = None\n",
      "2021-11-03 13:40:10,431 | INFO : args.gradient_accumulation_steps = 12\n",
      "2021-11-03 13:40:10,432 | INFO : args.eval_accumulation_steps = None\n",
      "2021-11-03 13:40:10,432 | INFO : args.learning_rate = 5e-05\n",
      "2021-11-03 13:40:10,432 | INFO : args.weight_decay = 0.0\n",
      "2021-11-03 13:40:10,433 | INFO : args.adam_beta1 = 0.9\n",
      "2021-11-03 13:40:10,433 | INFO : args.adam_beta2 = 0.999\n",
      "2021-11-03 13:40:10,433 | INFO : args.adam_epsilon = 1e-08\n",
      "2021-11-03 13:40:10,434 | INFO : args.max_grad_norm = 1.0\n",
      "2021-11-03 13:40:10,434 | INFO : args.num_train_epochs = 10\n",
      "2021-11-03 13:40:10,435 | INFO : args.max_steps = -1\n",
      "2021-11-03 13:40:10,435 | INFO : args.lr_scheduler_type = linear\n",
      "2021-11-03 13:40:10,436 | INFO : args.warmup_ratio = 0.0\n",
      "2021-11-03 13:40:10,436 | INFO : args.warmup_steps = 500\n",
      "2021-11-03 13:40:10,436 | INFO : args.logging_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/checkpoints/logs\n",
      "2021-11-03 13:40:10,437 | INFO : args.logging_strategy = steps\n",
      "2021-11-03 13:40:10,437 | INFO : args.logging_first_step = False\n",
      "2021-11-03 13:40:10,438 | INFO : args.logging_steps = 500\n",
      "2021-11-03 13:40:10,438 | INFO : args.save_strategy = steps\n",
      "2021-11-03 13:40:10,438 | INFO : args.save_steps = 500\n",
      "2021-11-03 13:40:10,439 | INFO : args.save_total_limit = 1\n",
      "2021-11-03 13:40:10,439 | INFO : args.no_cuda = False\n",
      "2021-11-03 13:40:10,439 | INFO : args.seed = 42\n",
      "2021-11-03 13:40:10,440 | INFO : args.fp16 = False\n",
      "2021-11-03 13:40:10,440 | INFO : args.fp16_opt_level = O1\n",
      "2021-11-03 13:40:10,443 | INFO : args.fp16_backend = auto\n",
      "2021-11-03 13:40:10,443 | INFO : args.fp16_full_eval = False\n",
      "2021-11-03 13:40:10,444 | INFO : args.local_rank = -1\n",
      "2021-11-03 13:40:10,444 | INFO : args.tpu_num_cores = None\n",
      "2021-11-03 13:40:10,445 | INFO : args.tpu_metrics_debug = False\n",
      "2021-11-03 13:40:10,445 | INFO : args.debug = False\n",
      "2021-11-03 13:40:10,445 | INFO : args.dataloader_drop_last = False\n",
      "2021-11-03 13:40:10,446 | INFO : args.eval_steps = None\n",
      "2021-11-03 13:40:10,446 | INFO : args.dataloader_num_workers = 0\n",
      "2021-11-03 13:40:10,446 | INFO : args.past_index = -1\n",
      "2021-11-03 13:40:10,447 | INFO : args.run_name = None\n",
      "2021-11-03 13:40:10,447 | INFO : args.disable_tqdm = None\n",
      "2021-11-03 13:40:10,447 | INFO : args.remove_unused_columns = True\n",
      "2021-11-03 13:40:10,448 | INFO : args.label_names = ['start_positions', 'end_positions']\n",
      "2021-11-03 13:40:10,448 | INFO : args.load_best_model_at_end = False\n",
      "2021-11-03 13:40:10,449 | INFO : args.metric_for_best_model = None\n",
      "2021-11-03 13:40:10,449 | INFO : args.greater_is_better = None\n",
      "2021-11-03 13:40:10,449 | INFO : args.ignore_data_skip = False\n",
      "2021-11-03 13:40:10,450 | INFO : args.sharded_ddp = \n",
      "2021-11-03 13:40:10,450 | INFO : args.deepspeed = None\n",
      "2021-11-03 13:40:10,450 | INFO : args.label_smoothing_factor = 0.0\n",
      "2021-11-03 13:40:10,451 | INFO : args.adafactor = False\n",
      "2021-11-03 13:40:10,451 | INFO : args.group_by_length = False\n",
      "2021-11-03 13:40:10,451 | INFO : args.length_column_name = length\n",
      "2021-11-03 13:40:10,452 | INFO : args.report_to = None\n",
      "2021-11-03 13:40:10,452 | INFO : args.ddp_find_unused_parameters = None\n",
      "2021-11-03 13:40:10,452 | INFO : args.dataloader_pin_memory = True\n",
      "2021-11-03 13:40:10,452 | INFO : args.skip_memory_metrics = False\n",
      "2021-11-03 13:40:10,452 | INFO : args.mp_parameters = \n",
      "2021-11-03 13:40:10,453 | INFO : args.result_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/outputs\n",
      "2021-11-03 13:40:10,453 | INFO : Transformers v4.5.1 defaults `--report_to` to 'all', so we change it to 'tensorboard'.\n",
      "2021-11-03 13:40:10,456 | INFO : callbacks = None\n",
      "2021-11-03 13:40:10,456 | INFO : model_wrapper.type = question_answering\n",
      "2021-11-03 13:40:10,457 | INFO : model_wrapper.pretrained_model = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 13:40:14,586 | INFO : tokenizer_wrapper.type = question-answering\n",
      "2021-11-03 13:40:14,587 | INFO : tokenizer_wrapper.pretrained_tokenizer = None\n",
      "2021-11-03 13:40:20,894 | INFO : optimizer.type = huggingface_adamw\n",
      "2021-11-03 13:40:20,895 | INFO : optimizer.lr = 5e-05\n",
      "2021-11-03 13:40:20,895 | INFO : optimizer.betas = (0.9, 0.999)\n",
      "2021-11-03 13:40:20,896 | INFO : optimizer.eps = 1e-06\n",
      "2021-11-03 13:40:20,896 | INFO : optimizer.weight_decay = 0.01\n",
      "2021-11-03 13:40:20,896 | INFO : optimizer.correct_bias = True\n",
      "2021-11-03 13:40:20,897 | INFO : Done constructing parameter groups.\n",
      "2021-11-03 13:40:20,898 | INFO : Group 0: ['roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias'], {'weight_decay': 0}\n",
      "2021-11-03 13:40:20,898 | INFO : Group 1: ['roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight'], {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 13:40:20,899 | WARNING : When constructing parameter groups, LayerNorm\\\\.weight does not match any parameter name\n",
      "2021-11-03 13:40:20,900 | WARNING : When constructing parameter groups, layer_norm\\\\.weight does not match any parameter name\n",
      "2021-11-03 13:40:20,900 | INFO : Number of trainable parameters: 124057346\n",
      "2021-11-03 13:40:20,901 | INFO : dataset_loader.type = default\n",
      "2021-11-03 13:40:20,901 | INFO : dataset_loader.dataset_reader.type = default\n",
      "2021-11-03 13:40:20,902 | INFO : dataset_loader.dataset_reader.path = squad_qa_test_fixture\n",
      "2021-11-03 13:40:20,903 | INFO : dataset_loader.dataset_reader.name = None\n",
      "2021-11-03 13:40:20,903 | INFO : dataset_loader.dataset_reader.data_dir = None\n",
      "2021-11-03 13:40:20,903 | INFO : dataset_loader.dataset_reader.data_files = None\n",
      "2021-11-03 13:40:20,903 | INFO : dataset_loader.dataset_reader.split = None\n",
      "2021-11-03 13:40:20,904 | INFO : dataset_loader.dataset_reader.cache_dir = None\n",
      "2021-11-03 13:40:20,904 | INFO : dataset_loader.dataset_reader.features = None\n",
      "2021-11-03 13:40:20,904 | INFO : dataset_loader.dataset_reader.download_config = None\n",
      "2021-11-03 13:40:20,904 | INFO : dataset_loader.dataset_reader.download_mode = None\n",
      "2021-11-03 13:40:20,905 | INFO : dataset_loader.dataset_reader.ignore_verifications = False\n",
      "2021-11-03 13:40:20,905 | INFO : dataset_loader.dataset_reader.keep_in_memory = None\n",
      "2021-11-03 13:40:20,905 | INFO : dataset_loader.dataset_reader.save_infos = False\n",
      "2021-11-03 13:40:20,905 | INFO : dataset_loader.dataset_reader.revision = None\n",
      "2021-11-03 13:40:20,905 | INFO : dataset_loader.dataset_reader.use_auth_token = None\n",
      "2021-11-03 13:40:20,906 | INFO : dataset_loader.dataset_reader.task = None\n",
      "2021-11-03 13:40:20,906 | INFO : dataset_loader.dataset_reader.streaming = False\n",
      "2021-11-03 13:40:20,906 | INFO : dataset_loader.dataset_reader.script_version = deprecated\n",
      "2021-11-03 13:40:20,907 | INFO : dataset_loader.data_processor.type = squad-question-answering\n",
      "2021-11-03 13:40:20,907 | INFO : dataset_loader.data_processor.model_max_sequence_length = None\n",
      "2021-11-03 13:40:20,909 | INFO : dataset_loader.data_processor.label_mapper = None\n",
      "2021-11-03 13:40:20,910 | INFO : dataset_loader.data_adapter.type = question-answering\n",
      "2021-11-03 13:40:20,910 | INFO : dataset_loader.data_adapter.label_mapper = None\n",
      "2021-11-03 13:40:20,917 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fc4e85aff1400b8718a4bc862a4481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1e6fdd188c424e84ea855cdf4fc52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9041971e3ef9405382e3ea80940b9d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 13:40:21,038 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10051e1396fc43a6a87f8078655d73e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286909af75464033a4dc398b4e5a6c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8599af6bb4ada84d79c6b9da062ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 13:40:21,138 | INFO : data_collator.type = default\n",
      "2021-11-03 13:40:23,230 | WARNING : Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "2021-11-03 13:40:23,231 | INFO : Training/evaluation parameters TransformerTrainingArguments(output_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/checkpoints', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=12, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=500, logging_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/checkpoints/logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/checkpoints', disable_tqdm=False, remove_unused_columns=True, label_names=['start_positions', 'end_positions'], load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', result_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/roberta-base-training-example/outputs')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1013] 2021-11-03 13:40:23,309 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-11-03 13:40:23,310 >>   Num examples = 5\n",
      "[INFO|trainer.py:1015] 2021-11-03 13:40:23,310 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1016] 2021-11-03 13:40:23,310 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1017] 2021-11-03 13:40:23,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:1018] 2021-11-03 13:40:23,311 >>   Gradient Accumulation steps = 12\n",
      "[INFO|trainer.py:1019] 2021-11-03 13:40:23,311 >>   Total optimization steps = 10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 5.80 GiB total capacity; 1.22 GiB already allocated; 53.81 MiB free; 1.31 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27342/3144814853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m result = run_experiment(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mext_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/training/train.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(config_path, params_overrides, ext_vars)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m     40\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_experiment_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_overrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_experiment_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/training/train.py\u001b[0m in \u001b[0;36m_run_experiment_from_params\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0m_save_experiment_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_experiment_using_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/training/train.py\u001b[0m in \u001b[0;36mrun_experiment_using_trainer\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Saves the tokenizer too for easy upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 5.80 GiB total capacity; 1.22 GiB already allocated; 53.81 MiB free; 1.31 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "ext_vars = {\n",
    "    # Used to feed the jsonnet config file with file paths\n",
    "    \"OUTPUT_PATH\": OUTPUT_DIR,\n",
    "    \"CHECKPOINT_PATH\": CHECKPOINT_DIR\n",
    "}\n",
    "\n",
    "result = run_experiment(\n",
    "    config_path=CONFIG_PATH,\n",
    "    ext_vars=ext_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, usage of pipeline for inference is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trapper.pipelines.question_answering_pipeline import SquadQuestionAnsweringPipeline\n",
    "from trapper.pipelines.pipeline import create_pipeline_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Some helper functions for inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data: Union[str, Dict]):\n",
    "    if isinstance(data, str):\n",
    "        data = load_json(data)\n",
    "    data = data[\"data\"]\n",
    "    qa_samples = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = {}\n",
    "                sample[\"context\"] = paragraph[\"context\"]\n",
    "                sample[\"question\"] = qa[\"question\"]\n",
    "                sample[\"gold_answers\"] = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
    "                qa_samples.append(sample)\n",
    "\n",
    "    return qa_samples\n",
    "\n",
    "\n",
    "def prepare_samples_for_pipeline(samples: List[Dict]):\n",
    "    pipeline_samples = deepcopy(samples)\n",
    "    for i, sample in enumerate(pipeline_samples):\n",
    "        sample.pop(\"gold_answers\")\n",
    "        if \"id\" not in sample:\n",
    "            sample[\"id\"] = str(i)\n",
    "    return pipeline_samples\n",
    "\n",
    "\n",
    "def predict(pipeline, samples, **kwargs):\n",
    "    pipeline_samples = prepare_samples_for_pipeline(samples)\n",
    "    predictions = pipeline(pipeline_samples, **kwargs)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        samples[i][\"predicted_answer\"] = prediction[0][\"answer\"].text\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DEV = os.path.join(PROJECT_ROOT, \"test_fixtures/data/question_answering/squad_qa/dev.json\")\n",
    "EXPORT_PATH = os.path.join(WORKING_DIR, \"qa-outputs.json\")\n",
    "\n",
    "PRETRAINED_MODEL_PATH = OUTPUT_DIR\n",
    "EXPERIMENT_CONFIG = os.path.join(PRETRAINED_MODEL_PATH, \"experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_pipeline = create_pipeline_from_checkpoint(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    experiment_config_path=EXPERIMENT_CONFIG,\n",
    "    task=\"squad-question-answering\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(SQUAD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predict(qa_pipeline, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(predictions, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [sample[\"gold_answers\"] for sample in predictions]\n",
    "hypotheses = [sample[\"predicted_answer\"] for sample in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury = Jury(metrics=\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jury.evaluate(references=references, predictions=hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
