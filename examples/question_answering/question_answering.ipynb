{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook serves as a walkthrough for training with trapper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Setting up the correct path\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "from jury import Jury\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trapper.training.train import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# You can customize your logger below.\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EXPERIMENT_NAME = \"roberta-base-training-example\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(WORKING_DIR))\n",
    "EXPERIMENTS_DIR = os.path.join(WORKING_DIR, \"experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Some useful helper functions to ease training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_from_task(path: str, task: str):\n",
    "    task = \"unnamed-task\" if task is None else task\n",
    "    return path.format(task=task)\n",
    "\n",
    "def start_experiment(config: str, task: str, ext_vars: Dict[str, str]):\n",
    "    result = run_experiment(\n",
    "        config_path=config,\n",
    "        ext_vars=ext_vars,\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TASK = \"question-answering\"\n",
    "TASK_DIR = get_dir_from_task(os.path.join(EXPERIMENTS_DIR, \"{task}\"), task=TASK)\n",
    "DATASET_DIR = os.path.join(TASK_DIR, \"datasets\")\n",
    "EXPERIMENT_DIR = os.path.join(TASK_DIR, EXPERIMENT_NAME)\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_DIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(EXPERIMENT_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ext_vars = {\n",
    "    # Used to feed the jsonnet config file with file paths\n",
    "    \"OUTPUT_PATH\": OUTPUT_DIR,\n",
    "    \"CHECKPOINT_PATH\": CHECKPOINT_DIR\n",
    "}\n",
    "\n",
    "CONFIG_PATH = os.path.join(TASK_DIR, \"experiment.jsonnet\")  # default experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:40:55,492 | INFO : type = default\n",
      "2021-11-02 13:40:55,492 | INFO : pretrained_model_name_or_path = roberta-base\n",
      "2021-11-02 13:40:55,493 | INFO : train_split_name = train\n",
      "2021-11-02 13:40:55,493 | INFO : dev_split_name = validation\n",
      "2021-11-02 13:40:55,494 | INFO : label_mapper = None\n",
      "2021-11-02 13:40:55,494 | INFO : compute_metrics = None\n",
      "2021-11-02 13:40:55,494 | INFO : no_grad = None\n",
      "2021-11-02 13:40:55,494 | INFO : args.type = default\n",
      "2021-11-02 13:40:55,495 | INFO : args.output_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints\n",
      "2021-11-02 13:40:55,495 | INFO : args.overwrite_output_dir = False\n",
      "2021-11-02 13:40:55,495 | INFO : args.do_train = True\n",
      "2021-11-02 13:40:55,496 | INFO : args.do_eval = True\n",
      "2021-11-02 13:40:55,496 | INFO : args.do_predict = False\n",
      "2021-11-02 13:40:55,496 | INFO : args.evaluation_strategy = steps\n",
      "2021-11-02 13:40:55,496 | INFO : args.prediction_loss_only = False\n",
      "2021-11-02 13:40:55,497 | INFO : args.per_device_train_batch_size = 2\n",
      "2021-11-02 13:40:55,497 | INFO : args.per_device_eval_batch_size = 2\n",
      "2021-11-02 13:40:55,497 | INFO : args.per_gpu_train_batch_size = None\n",
      "2021-11-02 13:40:55,497 | INFO : args.per_gpu_eval_batch_size = None\n",
      "2021-11-02 13:40:55,497 | INFO : args.gradient_accumulation_steps = 12\n",
      "2021-11-02 13:40:55,498 | INFO : args.eval_accumulation_steps = None\n",
      "2021-11-02 13:40:55,498 | INFO : args.learning_rate = 5e-05\n",
      "2021-11-02 13:40:55,498 | INFO : args.weight_decay = 0.0\n",
      "2021-11-02 13:40:55,498 | INFO : args.adam_beta1 = 0.9\n",
      "2021-11-02 13:40:55,499 | INFO : args.adam_beta2 = 0.999\n",
      "2021-11-02 13:40:55,499 | INFO : args.adam_epsilon = 1e-08\n",
      "2021-11-02 13:40:55,499 | INFO : args.max_grad_norm = 1.0\n",
      "2021-11-02 13:40:55,499 | INFO : args.num_train_epochs = 10\n",
      "2021-11-02 13:40:55,499 | INFO : args.max_steps = -1\n",
      "2021-11-02 13:40:55,500 | INFO : args.lr_scheduler_type = linear\n",
      "2021-11-02 13:40:55,500 | INFO : args.warmup_ratio = 0.0\n",
      "2021-11-02 13:40:55,500 | INFO : args.warmup_steps = 500\n",
      "2021-11-02 13:40:55,500 | INFO : args.logging_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints/logs\n",
      "2021-11-02 13:40:55,501 | INFO : args.logging_strategy = steps\n",
      "2021-11-02 13:40:55,501 | INFO : args.logging_first_step = False\n",
      "2021-11-02 13:40:55,501 | INFO : args.logging_steps = 500\n",
      "2021-11-02 13:40:55,501 | INFO : args.save_strategy = steps\n",
      "2021-11-02 13:40:55,501 | INFO : args.save_steps = 500\n",
      "2021-11-02 13:40:55,502 | INFO : args.save_total_limit = 1\n",
      "2021-11-02 13:40:55,502 | INFO : args.no_cuda = False\n",
      "2021-11-02 13:40:55,502 | INFO : args.seed = 42\n",
      "2021-11-02 13:40:55,502 | INFO : args.fp16 = False\n",
      "2021-11-02 13:40:55,503 | INFO : args.fp16_opt_level = O1\n",
      "2021-11-02 13:40:55,503 | INFO : args.fp16_backend = auto\n",
      "2021-11-02 13:40:55,503 | INFO : args.fp16_full_eval = False\n",
      "2021-11-02 13:40:55,503 | INFO : args.local_rank = -1\n",
      "2021-11-02 13:40:55,503 | INFO : args.tpu_num_cores = None\n",
      "2021-11-02 13:40:55,504 | INFO : args.tpu_metrics_debug = False\n",
      "2021-11-02 13:40:55,504 | INFO : args.debug = False\n",
      "2021-11-02 13:40:55,504 | INFO : args.dataloader_drop_last = False\n",
      "2021-11-02 13:40:55,504 | INFO : args.eval_steps = None\n",
      "2021-11-02 13:40:55,504 | INFO : args.dataloader_num_workers = 0\n",
      "2021-11-02 13:40:55,505 | INFO : args.past_index = -1\n",
      "2021-11-02 13:40:55,505 | INFO : args.run_name = None\n",
      "2021-11-02 13:40:55,505 | INFO : args.disable_tqdm = None\n",
      "2021-11-02 13:40:55,505 | INFO : args.remove_unused_columns = True\n",
      "2021-11-02 13:40:55,505 | INFO : args.label_names = ['start_positions', 'end_positions']\n",
      "2021-11-02 13:40:55,506 | INFO : args.load_best_model_at_end = False\n",
      "2021-11-02 13:40:55,506 | INFO : args.metric_for_best_model = None\n",
      "2021-11-02 13:40:55,506 | INFO : args.greater_is_better = None\n",
      "2021-11-02 13:40:55,506 | INFO : args.ignore_data_skip = False\n",
      "2021-11-02 13:40:55,506 | INFO : args.sharded_ddp = \n",
      "2021-11-02 13:40:55,507 | INFO : args.deepspeed = None\n",
      "2021-11-02 13:40:55,507 | INFO : args.label_smoothing_factor = 0.0\n",
      "2021-11-02 13:40:55,507 | INFO : args.adafactor = False\n",
      "2021-11-02 13:40:55,507 | INFO : args.group_by_length = False\n",
      "2021-11-02 13:40:55,507 | INFO : args.length_column_name = length\n",
      "2021-11-02 13:40:55,508 | INFO : args.report_to = None\n",
      "2021-11-02 13:40:55,508 | INFO : args.ddp_find_unused_parameters = None\n",
      "2021-11-02 13:40:55,508 | INFO : args.dataloader_pin_memory = True\n",
      "2021-11-02 13:40:55,508 | INFO : args.skip_memory_metrics = False\n",
      "2021-11-02 13:40:55,509 | INFO : args.mp_parameters = \n",
      "2021-11-02 13:40:55,509 | INFO : args.result_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "2021-11-02 13:40:55,509 | INFO : Transformers v4.5.1 defaults `--report_to` to 'all', so we change it to 'tensorboard'.\n",
      "2021-11-02 13:40:55,514 | INFO : callbacks = None\n",
      "2021-11-02 13:40:55,514 | INFO : model_wrapper.type = question_answering\n",
      "2021-11-02 13:40:55,515 | INFO : model_wrapper.pretrained_model = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:40:59,488 | INFO : tokenizer_wrapper.type = question-answering\n",
      "2021-11-02 13:40:59,489 | INFO : tokenizer_wrapper.pretrained_tokenizer = None\n",
      "2021-11-02 13:41:05,416 | INFO : optimizer.type = huggingface_adamw\n",
      "2021-11-02 13:41:05,417 | INFO : optimizer.lr = 5e-05\n",
      "2021-11-02 13:41:05,417 | INFO : optimizer.betas = (0.9, 0.999)\n",
      "2021-11-02 13:41:05,418 | INFO : optimizer.eps = 1e-06\n",
      "2021-11-02 13:41:05,418 | INFO : optimizer.weight_decay = 0.01\n",
      "2021-11-02 13:41:05,418 | INFO : optimizer.correct_bias = True\n",
      "2021-11-02 13:41:05,419 | INFO : Done constructing parameter groups.\n",
      "2021-11-02 13:41:05,419 | INFO : Group 0: ['roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.bias'], {'weight_decay': 0}\n",
      "2021-11-02 13:41:05,420 | INFO : Group 1: ['roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'qa_outputs.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.6.intermediate.dense.weight'], {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:41:05,420 | WARNING : When constructing parameter groups, LayerNorm\\\\.weight does not match any parameter name\n",
      "2021-11-02 13:41:05,420 | WARNING : When constructing parameter groups, layer_norm\\\\.weight does not match any parameter name\n",
      "2021-11-02 13:41:05,421 | INFO : Number of trainable parameters: 124057346\n",
      "2021-11-02 13:41:05,421 | INFO : dataset_loader.type = default\n",
      "2021-11-02 13:41:05,422 | INFO : dataset_loader.dataset_reader.type = default\n",
      "2021-11-02 13:41:05,424 | INFO : dataset_loader.dataset_reader.path = ../../test_fixtures/hf_datasets/squad_qa_test_fixture\n",
      "2021-11-02 13:41:05,424 | INFO : dataset_loader.dataset_reader.name = None\n",
      "2021-11-02 13:41:05,424 | INFO : dataset_loader.dataset_reader.data_dir = None\n",
      "2021-11-02 13:41:05,425 | INFO : dataset_loader.dataset_reader.data_files = None\n",
      "2021-11-02 13:41:05,425 | INFO : dataset_loader.dataset_reader.split = None\n",
      "2021-11-02 13:41:05,426 | INFO : dataset_loader.dataset_reader.cache_dir = None\n",
      "2021-11-02 13:41:05,426 | INFO : dataset_loader.dataset_reader.features = None\n",
      "2021-11-02 13:41:05,426 | INFO : dataset_loader.dataset_reader.download_config = None\n",
      "2021-11-02 13:41:05,427 | INFO : dataset_loader.dataset_reader.download_mode = None\n",
      "2021-11-02 13:41:05,427 | INFO : dataset_loader.dataset_reader.ignore_verifications = False\n",
      "2021-11-02 13:41:05,427 | INFO : dataset_loader.dataset_reader.keep_in_memory = None\n",
      "2021-11-02 13:41:05,428 | INFO : dataset_loader.dataset_reader.save_infos = False\n",
      "2021-11-02 13:41:05,428 | INFO : dataset_loader.dataset_reader.revision = None\n",
      "2021-11-02 13:41:05,429 | INFO : dataset_loader.dataset_reader.use_auth_token = None\n",
      "2021-11-02 13:41:05,429 | INFO : dataset_loader.dataset_reader.task = None\n",
      "2021-11-02 13:41:05,429 | INFO : dataset_loader.dataset_reader.streaming = False\n",
      "2021-11-02 13:41:05,430 | INFO : dataset_loader.dataset_reader.script_version = deprecated\n",
      "2021-11-02 13:41:05,430 | INFO : dataset_loader.data_processor.type = squad-question-answering\n",
      "2021-11-02 13:41:05,431 | INFO : dataset_loader.data_processor.model_max_sequence_length = None\n",
      "2021-11-02 13:41:05,431 | INFO : dataset_loader.data_processor.label_mapper = None\n",
      "2021-11-02 13:41:05,432 | INFO : dataset_loader.data_adapter.type = question-answering\n",
      "2021-11-02 13:41:05,432 | INFO : dataset_loader.data_adapter.label_mapper = None\n",
      "2021-11-02 13:41:05,446 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n",
      "2021-11-02 13:41:05,469 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-0a413bc0ba141c3c.arrow\n",
      "2021-11-02 13:41:05,473 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-d246ed715b9e5eda.arrow\n",
      "2021-11-02 13:41:05,501 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-f48fd04ab5f5b0cc.arrow\n",
      "2021-11-02 13:41:05,507 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n",
      "2021-11-02 13:41:05,530 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-039857ec89448cc5.arrow\n",
      "2021-11-02 13:41:05,533 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-5f6e088aa0596f90.arrow\n",
      "2021-11-02 13:41:05,554 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-cd27ee15c36c7b63.arrow\n",
      "2021-11-02 13:41:05,556 | INFO : data_collator.type = default\n",
      "2021-11-02 13:41:07,740 | WARNING : Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "2021-11-02 13:41:07,741 | INFO : Training/evaluation parameters TransformerTrainingArguments(output_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=12, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=500, logging_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints/logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints', disable_tqdm=False, remove_unused_columns=True, label_names=['start_positions', 'end_positions'], load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', result_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1013] 2021-11-02 13:41:07,802 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-11-02 13:41:07,803 >>   Num examples = 5\n",
      "[INFO|trainer.py:1015] 2021-11-02 13:41:07,803 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1016] 2021-11-02 13:41:07,803 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1017] 2021-11-02 13:41:07,804 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:1018] 2021-11-02 13:41:07,804 >>   Gradient Accumulation steps = 12\n",
      "[INFO|trainer.py:1019] 2021-11-02 13:41:07,804 >>   Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1196] 2021-11-02 13:41:10,046 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1648] 2021-11-02 13:41:10,144 >> Saving model checkpoint to /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "[INFO|configuration_utils.py:329] 2021-11-02 13:41:10,144 >> Configuration saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-11-02 13:41:10,682 >> Model weights saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1901] 2021-11-02 13:41:10,683 >> tokenizer config file saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-11-02 13:41:10,683 >> Special tokens file saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:41:10,733 | INFO : ***** Train results *****\n",
      "2021-11-02 13:41:10,733 | INFO :   epoch = 10.0\n",
      "2021-11-02 13:41:10,734 | INFO :   init_mem_cpu_alloc_delta = 2296905728\n",
      "2021-11-02 13:41:10,734 | INFO :   init_mem_cpu_peaked_delta = 380686336\n",
      "2021-11-02 13:41:10,734 | INFO :   init_mem_gpu_alloc_delta = 497524736\n",
      "2021-11-02 13:41:10,734 | INFO :   init_mem_gpu_peaked_delta = 0\n",
      "2021-11-02 13:41:10,734 | INFO :   total_flos = 6018021854460.0\n",
      "2021-11-02 13:41:10,735 | INFO :   train_mem_cpu_alloc_delta = 11640832\n",
      "2021-11-02 13:41:10,735 | INFO :   train_mem_cpu_peaked_delta = 0\n",
      "2021-11-02 13:41:10,735 | INFO :   train_mem_gpu_alloc_delta = 1496595968\n",
      "2021-11-02 13:41:10,735 | INFO :   train_mem_gpu_peaked_delta = 419147776\n",
      "2021-11-02 13:41:10,736 | INFO :   train_runtime = 2.2421\n",
      "2021-11-02 13:41:10,736 | INFO :   train_samples_per_second = 4.46\n",
      "2021-11-02 13:41:10,737 | INFO : *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1865] 2021-11-02 13:41:10,805 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1866] 2021-11-02 13:41:10,806 >>   Num examples = 6\n",
      "[INFO|trainer.py:1867] 2021-11-02 13:41:10,806 >>   Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:41:10,955 | INFO : ***** Eval results *****\n",
      "2021-11-02 13:41:10,955 | INFO :   epoch = 10.0\n",
      "2021-11-02 13:41:10,955 | INFO :   eval_loss = 5.149980545043945\n",
      "2021-11-02 13:41:10,956 | INFO :   eval_mem_cpu_alloc_delta = 0\n",
      "2021-11-02 13:41:10,956 | INFO :   eval_mem_cpu_peaked_delta = 0\n",
      "2021-11-02 13:41:10,956 | INFO :   eval_mem_gpu_alloc_delta = 0\n",
      "2021-11-02 13:41:10,957 | INFO :   eval_mem_gpu_peaked_delta = 16803328\n",
      "2021-11-02 13:41:10,957 | INFO :   eval_runtime = 0.0757\n",
      "2021-11-02 13:41:10,957 | INFO :   eval_samples_per_second = 79.209\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "result = start_experiment(\n",
    "    config=CONFIG_PATH,\n",
    "    task=TASK,\n",
    "    ext_vars=ext_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.149980545043945,\n",
       " 'eval_runtime': 0.0757,\n",
       " 'eval_samples_per_second': 79.209,\n",
       " 'epoch': 10.0,\n",
       " 'eval_mem_cpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 16803328}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, usage of pipeline for inference is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trapper.pipelines.question_answering_pipeline import SquadQuestionAnsweringPipeline\n",
    "from trapper.pipelines.pipeline import create_pipeline_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Some helper functions for inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(samples: List[Dict], path: str):\n",
    "    with open(path, \"w\") as jf:\n",
    "        json.dump(samples, jf)\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\") as jf:\n",
    "        return json.load(jf)\n",
    "\n",
    "\n",
    "def prepare_samples(data: Union[str, Dict]):\n",
    "    if isinstance(data, str):\n",
    "        data = load_json(data)\n",
    "    data = data[\"data\"]\n",
    "    qa_samples = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = {}\n",
    "                sample[\"context\"] = paragraph[\"context\"]\n",
    "                sample[\"question\"] = qa[\"question\"]\n",
    "                sample[\"gold_answers\"] = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
    "                qa_samples.append(sample)\n",
    "\n",
    "    return qa_samples\n",
    "\n",
    "\n",
    "def prepare_samples_for_pipeline(samples: List[Dict]):\n",
    "    pipeline_samples = deepcopy(samples)\n",
    "    for i, sample in enumerate(pipeline_samples):\n",
    "        sample.pop(\"gold_answers\")\n",
    "        if \"id\" not in sample:\n",
    "            sample[\"id\"] = str(i)\n",
    "    return pipeline_samples\n",
    "\n",
    "\n",
    "def predict(pipeline, samples, **kwargs):\n",
    "    pipeline_samples = prepare_samples_for_pipeline(samples)\n",
    "    predictions = pipeline(pipeline_samples, **kwargs)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        samples[i][\"predicted_answer\"] = prediction[0][\"answer\"].text\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DEV = os.path.join(PROJECT_ROOT, \"test_fixtures/data/question_answering/squad_qa/dev.json\")\n",
    "EXPORT_PATH = os.path.join(WORKING_DIR, \"qa-outputs.json\")\n",
    "\n",
    "PRETRAINED_MODEL_PATH = OUTPUT_DIR\n",
    "EXPERIMENT_CONFIG = os.path.join(PRETRAINED_MODEL_PATH, \"experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 16:43:42,899 | INFO : type = question_answering\n",
      "2021-11-02 16:43:42,899 | INFO : pretrained_model = None\n",
      "2021-11-02 16:43:42,900 | INFO : pretrained_model_name_or_path = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "2021-11-02 16:43:45,201 | INFO : type = question-answering\n",
      "2021-11-02 16:43:45,202 | INFO : pretrained_tokenizer = None\n",
      "2021-11-02 16:43:45,202 | INFO : pretrained_model_name_or_path = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = create_pipeline_from_checkpoint(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    experiment_config_path=EXPERIMENT_CONFIG,\n",
    "    task=\"squad-question-answering\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(SQUAD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 42.95it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(qa_pipeline, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(predictions, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [sample[\"gold_answers\"] for sample in predictions]\n",
    "hypotheses = [sample[\"predicted_answer\"] for sample in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury = Jury(metrics=\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empty_predictions': 0,\n",
       " 'total_items': 6,\n",
       " 'squad': {'exact_match': 0.0, 'f1': 0.0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jury.evaluate(references=references, predictions=hypotheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
