{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook serves as a walkthrough for training with trapper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the correct path\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from jury import Jury\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trapper.training.train import run_experiment\n",
    "from trapper.common.notebook_utils import download_fixture_data, load_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_fixture_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# You can customize your logger below.\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EXPERIMENT_NAME = \"roberta-base-training-example\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(WORKING_DIR))\n",
    "EXPERIMENT_DIR = os.path.join(WORKING_DIR, EXPERIMENT_NAME)\n",
    "CONFIG_PATH = os.path.join(WORKING_DIR, \"experiment.jsonnet\")  # default experiment params\n",
    "\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_DIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(EXPERIMENT_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ext_vars = {\n",
    "    # Used to feed the jsonnet config file with file paths\n",
    "    \"OUTPUT_PATH\": OUTPUT_DIR,\n",
    "    \"CHECKPOINT_PATH\": CHECKPOINT_DIR\n",
    "}\n",
    "\n",
    "result = run_experiment(\n",
    "    config_path=CONFIG_PATH,\n",
    "    ext_vars=ext_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, usage of pipeline for inference is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trapper.pipelines.question_answering_pipeline import SquadQuestionAnsweringPipeline\n",
    "from trapper.pipelines.pipeline import create_pipeline_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Some helper functions for inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data: Union[str, Dict]):\n",
    "    if isinstance(data, str):\n",
    "        data = load_json(data)\n",
    "    data = data[\"data\"]\n",
    "    qa_samples = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = {}\n",
    "                sample[\"context\"] = paragraph[\"context\"]\n",
    "                sample[\"question\"] = qa[\"question\"]\n",
    "                sample[\"gold_answers\"] = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
    "                qa_samples.append(sample)\n",
    "\n",
    "    return qa_samples\n",
    "\n",
    "\n",
    "def prepare_samples_for_pipeline(samples: List[Dict]):\n",
    "    pipeline_samples = deepcopy(samples)\n",
    "    for i, sample in enumerate(pipeline_samples):\n",
    "        sample.pop(\"gold_answers\")\n",
    "        if \"id\" not in sample:\n",
    "            sample[\"id\"] = str(i)\n",
    "    return pipeline_samples\n",
    "\n",
    "\n",
    "def predict(pipeline, samples, **kwargs):\n",
    "    pipeline_samples = prepare_samples_for_pipeline(samples)\n",
    "    predictions = pipeline(pipeline_samples, **kwargs)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        samples[i][\"predicted_answer\"] = prediction[0][\"answer\"].text\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DEV = os.path.join(PROJECT_ROOT, \"test_fixtures/data/question_answering/squad_qa/dev.json\")\n",
    "EXPORT_PATH = os.path.join(WORKING_DIR, \"qa-outputs.json\")\n",
    "\n",
    "PRETRAINED_MODEL_PATH = OUTPUT_DIR\n",
    "EXPERIMENT_CONFIG = os.path.join(PRETRAINED_MODEL_PATH, \"experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_pipeline = create_pipeline_from_checkpoint(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    experiment_config_path=EXPERIMENT_CONFIG,\n",
    "    task=\"squad-question-answering\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(SQUAD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predict(qa_pipeline, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(predictions, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [sample[\"gold_answers\"] for sample in predictions]\n",
    "hypotheses = [sample[\"predicted_answer\"] for sample in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury = Jury(metrics=\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jury.evaluate(references=references, predictions=hypotheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
