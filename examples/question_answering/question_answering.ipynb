{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook serves as a walkthrough for training with trapper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Setting up the correct path\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "from jury import Jury\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trapper.training.train import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# You can customize your logger below.\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EXPERIMENT_NAME = \"roberta-base-training-example\"\n",
    "\n",
    "WORKING_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(WORKING_DIR))\n",
    "EXPERIMENTS_DIR = os.path.join(WORKING_DIR, \"experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Some useful helper functions to ease training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_from_task(path: str, task: str):\n",
    "    task = \"unnamed-task\" if task is None else task\n",
    "    return path.format(task=task)\n",
    "\n",
    "def start_experiment(config: str, task: str, ext_vars: Dict[str, str]):\n",
    "    result = run_experiment(\n",
    "        config_path=config,\n",
    "        ext_vars=ext_vars,\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TASK = \"question-answering\"\n",
    "TASK_DIR = get_dir_from_task(os.path.join(EXPERIMENTS_DIR, \"{task}\"), task=TASK)\n",
    "DATASET_DIR = os.path.join(TASK_DIR, \"datasets\")\n",
    "EXPERIMENT_DIR = os.path.join(TASK_DIR, EXPERIMENT_NAME)\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_DIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(EXPERIMENT_DIR, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ext_vars = {\n",
    "    # Used to feed the jsonnet config file with file paths\n",
    "    \"OUTPUT_PATH\": OUTPUT_DIR,\n",
    "    \"CHECKPOINT_PATH\": CHECKPOINT_DIR\n",
    "}\n",
    "\n",
    "CONFIG_PATH = os.path.join(TASK_DIR, \"experiment.jsonnet\")  # default experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:59:02,582 | INFO : type = default\n",
      "2021-11-02 17:59:02,583 | INFO : pretrained_model_name_or_path = roberta-base\n",
      "2021-11-02 17:59:02,583 | INFO : train_split_name = train\n",
      "2021-11-02 17:59:02,583 | INFO : dev_split_name = validation\n",
      "2021-11-02 17:59:02,584 | INFO : label_mapper = None\n",
      "2021-11-02 17:59:02,585 | INFO : compute_metrics = None\n",
      "2021-11-02 17:59:02,585 | INFO : no_grad = None\n",
      "2021-11-02 17:59:02,585 | INFO : args.type = default\n",
      "2021-11-02 17:59:02,586 | INFO : args.output_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints\n",
      "2021-11-02 17:59:02,586 | INFO : args.overwrite_output_dir = False\n",
      "2021-11-02 17:59:02,586 | INFO : args.do_train = True\n",
      "2021-11-02 17:59:02,586 | INFO : args.do_eval = True\n",
      "2021-11-02 17:59:02,587 | INFO : args.do_predict = False\n",
      "2021-11-02 17:59:02,587 | INFO : args.evaluation_strategy = steps\n",
      "2021-11-02 17:59:02,587 | INFO : args.prediction_loss_only = False\n",
      "2021-11-02 17:59:02,587 | INFO : args.per_device_train_batch_size = 2\n",
      "2021-11-02 17:59:02,588 | INFO : args.per_device_eval_batch_size = 2\n",
      "2021-11-02 17:59:02,588 | INFO : args.per_gpu_train_batch_size = None\n",
      "2021-11-02 17:59:02,588 | INFO : args.per_gpu_eval_batch_size = None\n",
      "2021-11-02 17:59:02,589 | INFO : args.gradient_accumulation_steps = 12\n",
      "2021-11-02 17:59:02,589 | INFO : args.eval_accumulation_steps = None\n",
      "2021-11-02 17:59:02,589 | INFO : args.learning_rate = 5e-05\n",
      "2021-11-02 17:59:02,590 | INFO : args.weight_decay = 0.0\n",
      "2021-11-02 17:59:02,590 | INFO : args.adam_beta1 = 0.9\n",
      "2021-11-02 17:59:02,590 | INFO : args.adam_beta2 = 0.999\n",
      "2021-11-02 17:59:02,590 | INFO : args.adam_epsilon = 1e-08\n",
      "2021-11-02 17:59:02,591 | INFO : args.max_grad_norm = 1.0\n",
      "2021-11-02 17:59:02,591 | INFO : args.num_train_epochs = 10\n",
      "2021-11-02 17:59:02,591 | INFO : args.max_steps = -1\n",
      "2021-11-02 17:59:02,591 | INFO : args.lr_scheduler_type = linear\n",
      "2021-11-02 17:59:02,592 | INFO : args.warmup_ratio = 0.0\n",
      "2021-11-02 17:59:02,592 | INFO : args.warmup_steps = 500\n",
      "2021-11-02 17:59:02,592 | INFO : args.logging_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints/logs\n",
      "2021-11-02 17:59:02,592 | INFO : args.logging_strategy = steps\n",
      "2021-11-02 17:59:02,592 | INFO : args.logging_first_step = False\n",
      "2021-11-02 17:59:02,593 | INFO : args.logging_steps = 500\n",
      "2021-11-02 17:59:02,593 | INFO : args.save_strategy = steps\n",
      "2021-11-02 17:59:02,593 | INFO : args.save_steps = 500\n",
      "2021-11-02 17:59:02,594 | INFO : args.save_total_limit = 1\n",
      "2021-11-02 17:59:02,594 | INFO : args.no_cuda = False\n",
      "2021-11-02 17:59:02,594 | INFO : args.seed = 42\n",
      "2021-11-02 17:59:02,594 | INFO : args.fp16 = False\n",
      "2021-11-02 17:59:02,595 | INFO : args.fp16_opt_level = O1\n",
      "2021-11-02 17:59:02,595 | INFO : args.fp16_backend = auto\n",
      "2021-11-02 17:59:02,595 | INFO : args.fp16_full_eval = False\n",
      "2021-11-02 17:59:02,596 | INFO : args.local_rank = -1\n",
      "2021-11-02 17:59:02,596 | INFO : args.tpu_num_cores = None\n",
      "2021-11-02 17:59:02,596 | INFO : args.tpu_metrics_debug = False\n",
      "2021-11-02 17:59:02,596 | INFO : args.debug = False\n",
      "2021-11-02 17:59:02,597 | INFO : args.dataloader_drop_last = False\n",
      "2021-11-02 17:59:02,597 | INFO : args.eval_steps = None\n",
      "2021-11-02 17:59:02,597 | INFO : args.dataloader_num_workers = 0\n",
      "2021-11-02 17:59:02,597 | INFO : args.past_index = -1\n",
      "2021-11-02 17:59:02,598 | INFO : args.run_name = None\n",
      "2021-11-02 17:59:02,598 | INFO : args.disable_tqdm = None\n",
      "2021-11-02 17:59:02,598 | INFO : args.remove_unused_columns = True\n",
      "2021-11-02 17:59:02,598 | INFO : args.label_names = ['start_positions', 'end_positions']\n",
      "2021-11-02 17:59:02,599 | INFO : args.load_best_model_at_end = False\n",
      "2021-11-02 17:59:02,599 | INFO : args.metric_for_best_model = None\n",
      "2021-11-02 17:59:02,599 | INFO : args.greater_is_better = None\n",
      "2021-11-02 17:59:02,599 | INFO : args.ignore_data_skip = False\n",
      "2021-11-02 17:59:02,599 | INFO : args.sharded_ddp = \n",
      "2021-11-02 17:59:02,600 | INFO : args.deepspeed = None\n",
      "2021-11-02 17:59:02,600 | INFO : args.label_smoothing_factor = 0.0\n",
      "2021-11-02 17:59:02,600 | INFO : args.adafactor = False\n",
      "2021-11-02 17:59:02,601 | INFO : args.group_by_length = False\n",
      "2021-11-02 17:59:02,601 | INFO : args.length_column_name = length\n",
      "2021-11-02 17:59:02,601 | INFO : args.report_to = None\n",
      "2021-11-02 17:59:02,601 | INFO : args.ddp_find_unused_parameters = None\n",
      "2021-11-02 17:59:02,601 | INFO : args.dataloader_pin_memory = True\n",
      "2021-11-02 17:59:02,602 | INFO : args.skip_memory_metrics = False\n",
      "2021-11-02 17:59:02,602 | INFO : args.mp_parameters = \n",
      "2021-11-02 17:59:02,602 | INFO : args.result_dir = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "2021-11-02 17:59:02,602 | INFO : Transformers v4.5.1 defaults `--report_to` to 'all', so we change it to 'tensorboard'.\n",
      "2021-11-02 17:59:02,609 | INFO : callbacks = None\n",
      "2021-11-02 17:59:02,609 | INFO : model_wrapper.type = question_answering\n",
      "2021-11-02 17:59:02,610 | INFO : model_wrapper.pretrained_model = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:59:07,064 | INFO : tokenizer_wrapper.type = question-answering\n",
      "2021-11-02 17:59:07,065 | INFO : tokenizer_wrapper.pretrained_tokenizer = None\n",
      "2021-11-02 17:59:13,182 | INFO : optimizer.type = huggingface_adamw\n",
      "2021-11-02 17:59:13,183 | INFO : optimizer.lr = 5e-05\n",
      "2021-11-02 17:59:13,183 | INFO : optimizer.betas = (0.9, 0.999)\n",
      "2021-11-02 17:59:13,183 | INFO : optimizer.eps = 1e-06\n",
      "2021-11-02 17:59:13,184 | INFO : optimizer.weight_decay = 0.01\n",
      "2021-11-02 17:59:13,184 | INFO : optimizer.correct_bias = True\n",
      "2021-11-02 17:59:13,185 | INFO : Done constructing parameter groups.\n",
      "2021-11-02 17:59:13,186 | INFO : Group 0: ['roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'qa_outputs.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias'], {'weight_decay': 0}\n",
      "2021-11-02 17:59:13,186 | INFO : Group 1: ['roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'qa_outputs.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight'], {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:59:13,187 | WARNING : When constructing parameter groups, LayerNorm\\\\.weight does not match any parameter name\n",
      "2021-11-02 17:59:13,187 | WARNING : When constructing parameter groups, layer_norm\\\\.weight does not match any parameter name\n",
      "2021-11-02 17:59:13,188 | INFO : Number of trainable parameters: 124057346\n",
      "2021-11-02 17:59:13,188 | INFO : dataset_loader.type = default\n",
      "2021-11-02 17:59:13,189 | INFO : dataset_loader.dataset_reader.type = default\n",
      "2021-11-02 17:59:13,190 | INFO : dataset_loader.dataset_reader.path = ../../test_fixtures/hf_datasets/squad_qa_test_fixture\n",
      "2021-11-02 17:59:13,191 | INFO : dataset_loader.dataset_reader.name = None\n",
      "2021-11-02 17:59:13,191 | INFO : dataset_loader.dataset_reader.data_dir = None\n",
      "2021-11-02 17:59:13,191 | INFO : dataset_loader.dataset_reader.data_files = None\n",
      "2021-11-02 17:59:13,191 | INFO : dataset_loader.dataset_reader.split = None\n",
      "2021-11-02 17:59:13,192 | INFO : dataset_loader.dataset_reader.cache_dir = None\n",
      "2021-11-02 17:59:13,192 | INFO : dataset_loader.dataset_reader.features = None\n",
      "2021-11-02 17:59:13,192 | INFO : dataset_loader.dataset_reader.download_config = None\n",
      "2021-11-02 17:59:13,194 | INFO : dataset_loader.dataset_reader.download_mode = None\n",
      "2021-11-02 17:59:13,194 | INFO : dataset_loader.dataset_reader.ignore_verifications = False\n",
      "2021-11-02 17:59:13,194 | INFO : dataset_loader.dataset_reader.keep_in_memory = None\n",
      "2021-11-02 17:59:13,194 | INFO : dataset_loader.dataset_reader.save_infos = False\n",
      "2021-11-02 17:59:13,195 | INFO : dataset_loader.dataset_reader.revision = None\n",
      "2021-11-02 17:59:13,195 | INFO : dataset_loader.dataset_reader.use_auth_token = None\n",
      "2021-11-02 17:59:13,195 | INFO : dataset_loader.dataset_reader.task = None\n",
      "2021-11-02 17:59:13,196 | INFO : dataset_loader.dataset_reader.streaming = False\n",
      "2021-11-02 17:59:13,196 | INFO : dataset_loader.dataset_reader.script_version = deprecated\n",
      "2021-11-02 17:59:13,196 | INFO : dataset_loader.data_processor.type = squad-question-answering\n",
      "2021-11-02 17:59:13,197 | INFO : dataset_loader.data_processor.model_max_sequence_length = None\n",
      "2021-11-02 17:59:13,197 | INFO : dataset_loader.data_processor.label_mapper = None\n",
      "2021-11-02 17:59:13,197 | INFO : dataset_loader.data_adapter.type = question-answering\n",
      "2021-11-02 17:59:13,198 | INFO : dataset_loader.data_adapter.label_mapper = None\n",
      "2021-11-02 17:59:13,207 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n",
      "2021-11-02 17:59:13,235 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-0a413bc0ba141c3c.arrow\n",
      "2021-11-02 17:59:13,238 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-d246ed715b9e5eda.arrow\n",
      "2021-11-02 17:59:13,263 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-f48fd04ab5f5b0cc.arrow\n",
      "2021-11-02 17:59:13,270 | WARNING : Reusing dataset squad_test_fixture (/home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e)\n",
      "2021-11-02 17:59:13,291 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-039857ec89448cc5.arrow\n",
      "2021-11-02 17:59:13,293 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-5f6e088aa0596f90.arrow\n",
      "2021-11-02 17:59:13,316 | WARNING : Loading cached processed dataset at /home/devrimcavusoglu/.cache/huggingface/datasets/squad_test_fixture/qa_test_fixture/1.0.0/c58d8440c9470bf281cc8be01736a48d7e0eb11a25067d29461b83b51132304e/cache-cd27ee15c36c7b63.arrow\n",
      "2021-11-02 17:59:13,318 | INFO : data_collator.type = default\n",
      "2021-11-02 17:59:16,062 | WARNING : Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "2021-11-02 17:59:16,063 | INFO : Training/evaluation parameters TransformerTrainingArguments(output_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=12, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=500, logging_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints/logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/checkpoints', disable_tqdm=False, remove_unused_columns=True, label_names=['start_positions', 'end_positions'], load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', result_dir='/home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1013] 2021-11-02 17:59:16,126 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-11-02 17:59:16,127 >>   Num examples = 5\n",
      "[INFO|trainer.py:1015] 2021-11-02 17:59:16,127 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1016] 2021-11-02 17:59:16,127 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1017] 2021-11-02 17:59:16,128 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:1018] 2021-11-02 17:59:16,128 >>   Gradient Accumulation steps = 12\n",
      "[INFO|trainer.py:1019] 2021-11-02 17:59:16,129 >>   Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1196] 2021-11-02 17:59:18,416 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1648] 2021-11-02 17:59:18,508 >> Saving model checkpoint to /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "[INFO|configuration_utils.py:329] 2021-11-02 17:59:18,509 >> Configuration saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/config.json\n",
      "[INFO|modeling_utils.py:831] 2021-11-02 17:59:19,018 >> Model weights saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1901] 2021-11-02 17:59:19,020 >> tokenizer config file saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1907] 2021-11-02 17:59:19,020 >> Special tokens file saved in /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:59:19,083 | INFO : ***** Train results *****\n",
      "2021-11-02 17:59:19,084 | INFO :   epoch = 10.0\n",
      "2021-11-02 17:59:19,084 | INFO :   init_mem_cpu_alloc_delta = 2318487552\n",
      "2021-11-02 17:59:19,084 | INFO :   init_mem_cpu_peaked_delta = 380424192\n",
      "2021-11-02 17:59:19,085 | INFO :   init_mem_gpu_alloc_delta = 497524736\n",
      "2021-11-02 17:59:19,085 | INFO :   init_mem_gpu_peaked_delta = 0\n",
      "2021-11-02 17:59:19,085 | INFO :   total_flos = 6018021854460.0\n",
      "2021-11-02 17:59:19,086 | INFO :   train_mem_cpu_alloc_delta = 13774848\n",
      "2021-11-02 17:59:19,086 | INFO :   train_mem_cpu_peaked_delta = 0\n",
      "2021-11-02 17:59:19,087 | INFO :   train_mem_gpu_alloc_delta = 1496595968\n",
      "2021-11-02 17:59:19,087 | INFO :   train_mem_gpu_peaked_delta = 419147776\n",
      "2021-11-02 17:59:19,088 | INFO :   train_runtime = 2.2877\n",
      "2021-11-02 17:59:19,088 | INFO :   train_samples_per_second = 4.371\n",
      "2021-11-02 17:59:19,089 | INFO : *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1865] 2021-11-02 17:59:19,166 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1866] 2021-11-02 17:59:19,166 >>   Num examples = 6\n",
      "[INFO|trainer.py:1867] 2021-11-02 17:59:19,166 >>   Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 17:59:19,307 | INFO : ***** Eval results *****\n",
      "2021-11-02 17:59:19,307 | INFO :   epoch = 10.0\n",
      "2021-11-02 17:59:19,307 | INFO :   eval_loss = 5.103574275970459\n",
      "2021-11-02 17:59:19,308 | INFO :   eval_mem_cpu_alloc_delta = 0\n",
      "2021-11-02 17:59:19,308 | INFO :   eval_mem_cpu_peaked_delta = 0\n",
      "2021-11-02 17:59:19,308 | INFO :   eval_mem_gpu_alloc_delta = 0\n",
      "2021-11-02 17:59:19,308 | INFO :   eval_mem_gpu_peaked_delta = 16803328\n",
      "2021-11-02 17:59:19,309 | INFO :   eval_runtime = 0.0801\n",
      "2021-11-02 17:59:19,309 | INFO :   eval_samples_per_second = 74.936\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "result = start_experiment(\n",
    "    config=CONFIG_PATH,\n",
    "    task=TASK,\n",
    "    ext_vars=ext_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.103574275970459,\n",
       " 'eval_runtime': 0.0801,\n",
       " 'eval_samples_per_second': 74.936,\n",
       " 'epoch': 10.0,\n",
       " 'eval_mem_cpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 16803328}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "In this section, usage of pipeline for inference is illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trapper.pipelines.question_answering_pipeline import SquadQuestionAnsweringPipeline\n",
    "from trapper.pipelines.pipeline import create_pipeline_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Some helper functions for inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(samples: List[Dict], path: str):\n",
    "    with open(path, \"w\") as jf:\n",
    "        json.dump(samples, jf)\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\") as jf:\n",
    "        return json.load(jf)\n",
    "\n",
    "\n",
    "def prepare_samples(data: Union[str, Dict]):\n",
    "    if isinstance(data, str):\n",
    "        data = load_json(data)\n",
    "    data = data[\"data\"]\n",
    "    qa_samples = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = {}\n",
    "                sample[\"context\"] = paragraph[\"context\"]\n",
    "                sample[\"question\"] = qa[\"question\"]\n",
    "                sample[\"gold_answers\"] = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
    "                qa_samples.append(sample)\n",
    "\n",
    "    return qa_samples\n",
    "\n",
    "\n",
    "def prepare_samples_for_pipeline(samples: List[Dict]):\n",
    "    pipeline_samples = deepcopy(samples)\n",
    "    for i, sample in enumerate(pipeline_samples):\n",
    "        sample.pop(\"gold_answers\")\n",
    "        if \"id\" not in sample:\n",
    "            sample[\"id\"] = str(i)\n",
    "    return pipeline_samples\n",
    "\n",
    "\n",
    "def predict(pipeline, samples, **kwargs):\n",
    "    pipeline_samples = prepare_samples_for_pipeline(samples)\n",
    "    predictions = pipeline(pipeline_samples, **kwargs)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        samples[i][\"predicted_answer\"] = prediction[0][\"answer\"].text\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DEV = os.path.join(PROJECT_ROOT, \"test_fixtures/data/question_answering/squad_qa/dev.json\")\n",
    "EXPORT_PATH = os.path.join(WORKING_DIR, \"qa-outputs.json\")\n",
    "\n",
    "PRETRAINED_MODEL_PATH = OUTPUT_DIR\n",
    "EXPERIMENT_CONFIG = os.path.join(PRETRAINED_MODEL_PATH, \"experiment_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 18:04:43,935 | INFO : type = question_answering\n",
      "2021-11-02 18:04:43,935 | INFO : pretrained_model = None\n",
      "2021-11-02 18:04:43,936 | INFO : pretrained_model_name_or_path = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n",
      "2021-11-02 18:04:46,138 | INFO : type = question-answering\n",
      "2021-11-02 18:04:46,139 | INFO : pretrained_tokenizer = None\n",
      "2021-11-02 18:04:46,140 | INFO : pretrained_model_name_or_path = /home/devrimcavusoglu/lab/gh/trapper/examples/question_answering/experiments/question-answering/roberta-base-training-example/outputs\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = create_pipeline_from_checkpoint(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    experiment_config_path=EXPERIMENT_CONFIG,\n",
    "    task=\"squad-question-answering\",\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(SQUAD_DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 3/6 [00:00<00:00, 27.18it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41177/1159774807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_41177/3384799391.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(pipeline, samples, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpipeline_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_samples_for_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_answer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/pipelines/question_answering_pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         {\n\u001b[1;32m    270\u001b[0m                             \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                             \"answer\": self._construct_answer(\n\u001b[0m\u001b[1;32m    272\u001b[0m                                 \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/pipelines/question_answering_pipeline.py\u001b[0m in \u001b[0;36m_construct_answer\u001b[0;34m(self, context, input_ids, start_token_ind, end_token_ind)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mend_token_ind\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     ) -> SpanTuple:\n\u001b[0;32m--> 354\u001b[0;31m         answer_start_ind = self._get_answer_start_ind(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token_ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trapper/pipelines/question_answering_pipeline.py\u001b[0m in \u001b[0;36m_get_answer_start_ind\u001b[0;34m(self, context, input_ids, start_token_ind)\u001b[0m\n\u001b[1;32m    376\u001b[0m         )\n\u001b[1;32m    377\u001b[0m         \u001b[0manswer_start_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_start_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0manswer_start_ind\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manswer_start_ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "predictions = predict(qa_pipeline, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(predictions, EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [sample[\"gold_answers\"] for sample in predictions]\n",
    "hypotheses = [sample[\"predicted_answer\"] for sample in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury = Jury(metrics=\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empty_predictions': 0,\n",
       " 'total_items': 6,\n",
       " 'squad': {'exact_match': 0.0, 'f1': 0.0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jury.evaluate(references=references, predictions=hypotheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
